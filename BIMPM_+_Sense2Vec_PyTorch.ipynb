{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BIMPM + Sense2Vec PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWjSRvrKtEX1",
        "colab_type": "code",
        "outputId": "8f1c3058-0ad2-47bd-80f6-6dcd61d45152",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        " !pip3 install nltk==3.2.4\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk==3.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c2/858e0708b497116ae45cf5c6b1f66984ac60729c61e49df6c1c0b808d1e4/nltk-3.2.4.tar.gz (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk==3.2.4) (1.12.0)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.2.4-cp36-none-any.whl size=1367703 sha256=d3aceb1f6a2852facbf3feb5d7c1fe571833d020745031f4b09bdca4777b08a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/f1/5c/f667347d86a3a534ba4c0127eed4389f929916e3ec88bb461a\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vCQ-DRstIjc",
        "colab_type": "code",
        "outputId": "44a6ac6e-5237-4196-b6db-c5d9a4d75a50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "!pip3 install torchtext==0.2.0\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/73/ac7461744aad1685595e112958555e1c8bc460e01d11047467b23521eb43/torchtext-0.2.0-py3-none-any.whl (40kB)\n",
            "\r\u001b[K     |████████                        | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 3.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.0) (4.28.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.2.0) (2.21.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.0) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.2.0) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed torchtext-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GN9QFcImtSqR",
        "colab_type": "code",
        "outputId": "dfb494e6-b988-4372-b98e-1bd8fe6e880c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        }
      },
      "source": [
        "!pip3 install torch==0.3.1\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==0.3.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/a5/e8b50b55b1abac9f1e3346c4242f1e42a82d368a8442cbd50c532922f6c4/torch-0.3.1-cp36-cp36m-manylinux1_x86_64.whl (496.4MB)\n",
            "\u001b[K     |████████████████████████████████| 496.4MB 29kB/s \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viTEcODMtYu2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install tensorboardX==0.8["
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inRyW5mYuIif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "\n",
        "def test(model, args, data, mode='test'):\n",
        "    if mode == 'dev':\n",
        "        iterator = iter(data.dev_iter)\n",
        "    else:\n",
        "        iterator = iter(data.test_iter)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    acc, loss, size = 0, 0, 0\n",
        "\n",
        "    for batch in iterator:\n",
        "        if args[\"data_type\"] == 'SNLI':\n",
        "            s1, s2 = 'premise', 'hypothesis'\n",
        "        else:\n",
        "            s1, s2 = 'q1', 'q2'\n",
        "\n",
        "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
        "        kwargs = {'p': s1, 'h': s2}\n",
        "\n",
        "        if args[\"use_char_emb\"]:\n",
        "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
        "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
        "\n",
        "            if args[\"gpu\"] > -1:\n",
        "                char_p = char_p.cuda(args[\"gpu\"])\n",
        "                char_h = char_h.cuda(args[\"gpu\"])\n",
        "\n",
        "            kwargs['char_p'] = char_p\n",
        "            kwargs['char_h'] = char_h\n",
        "\n",
        "        pred = model(**kwargs)\n",
        "\n",
        "        batch_loss = criterion(pred, batch.label)\n",
        "        loss += batch_loss.data[0]\n",
        "\n",
        "        _, pred = pred.max(dim=1)\n",
        "        acc += (pred == batch.label).sum().float()\n",
        "        size += len(pred)\n",
        "\n",
        "    acc /= size\n",
        "    acc = acc.cpu().data[0]\n",
        "    return loss, acc\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH26_pnqvsQa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BIMPM(nn.Module):\n",
        "    def __init__(self, args, data):\n",
        "        super(BIMPM, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        self.d = self.args[\"word_dim\"] + int(self.args[\"use_char_emb\"]) * self.args[\"char_hidden_size\"]\n",
        "        self.l = self.args[\"num_perspective\"]\n",
        "\n",
        "        # ----- Word Representation Layer -----\n",
        "        self.char_emb = nn.Embedding(args[\"char_vocab_size\"], args[\"char_dim\"], padding_idx=0)\n",
        "\n",
        "        self.word_emb = nn.Embedding(args[\"word_vocab_size\"], args[\"word_dim\"])\n",
        "        # initialize word embedding with GloVe\n",
        "        self.word_emb.weight.data.copy_(data.TEXT1.vocab.vectors)\n",
        "        # no fine-tuning for word vectors\n",
        "        self.word_emb.weight.requires_grad = False\n",
        "\n",
        "        self.char_LSTM = nn.LSTM(\n",
        "            input_size=self.args[\"char_dim\"],\n",
        "            hidden_size=self.args[\"char_hidden_size\"],\n",
        "            num_layers=1,\n",
        "            bidirectional=False,\n",
        "            batch_first=True)\n",
        "\n",
        "        # ----- Context Representation Layer -----\n",
        "        self.context_LSTM = nn.LSTM(\n",
        "            input_size=self.d,\n",
        "            hidden_size=self.args[\"hidden_size\"],\n",
        "            num_layers=1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # ----- Matching Layer -----\n",
        "        for i in range(1, 9):\n",
        "            setattr(self, f'mp_w{i}',\n",
        "                    nn.Parameter(torch.rand(self.l, self.args[\"hidden_size\"])))\n",
        "\n",
        "        # ----- Aggregation Layer -----\n",
        "        self.aggregation_LSTM = nn.LSTM(\n",
        "            input_size=self.l * 8,\n",
        "            hidden_size=self.args[\"hidden_size\"],\n",
        "            num_layers=1,\n",
        "            bidirectional=True,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # ----- Prediction Layer -----\n",
        "        self.pred_fc1 = nn.Linear(self.args[\"hidden_size\"] * 4, self.args[\"hidden_size\"] * 2)\n",
        "        self.pred_fc2 = nn.Linear(self.args[\"hidden_size\"]* 2, self.args[\"class_size\"])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # ----- Word Representation Layer -----\n",
        "        nn.init.uniform(self.char_emb.weight, -0.005, 0.005)\n",
        "        # zero vectors for padding\n",
        "        self.char_emb.weight.data[0].fill_(0)\n",
        "\n",
        "        # <unk> vectors is randomly initialized\n",
        "        nn.init.uniform(self.word_emb.weight.data[0], -0.1, 0.1)\n",
        "\n",
        "        nn.init.kaiming_normal(self.char_LSTM.weight_ih_l0)\n",
        "        nn.init.constant(self.char_LSTM.bias_ih_l0, val=0)\n",
        "        nn.init.orthogonal(self.char_LSTM.weight_hh_l0)\n",
        "        nn.init.constant(self.char_LSTM.bias_hh_l0, val=0)\n",
        "\n",
        "        # ----- Context Representation Layer -----\n",
        "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0)\n",
        "        nn.init.constant(self.context_LSTM.bias_ih_l0, val=0)\n",
        "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0)\n",
        "        nn.init.constant(self.context_LSTM.bias_hh_l0, val=0)\n",
        "\n",
        "        nn.init.kaiming_normal(self.context_LSTM.weight_ih_l0_reverse)\n",
        "        nn.init.constant(self.context_LSTM.bias_ih_l0_reverse, val=0)\n",
        "        nn.init.orthogonal(self.context_LSTM.weight_hh_l0_reverse)\n",
        "        nn.init.constant(self.context_LSTM.bias_hh_l0_reverse, val=0)\n",
        "\n",
        "        # ----- Matching Layer -----\n",
        "        for i in range(1, 9):\n",
        "            w = getattr(self, f'mp_w{i}')\n",
        "            nn.init.kaiming_normal(w)\n",
        "\n",
        "        # ----- Aggregation Layer -----\n",
        "        nn.init.kaiming_normal(self.aggregation_LSTM.weight_ih_l0)\n",
        "        nn.init.constant(self.aggregation_LSTM.bias_ih_l0, val=0)\n",
        "        nn.init.orthogonal(self.aggregation_LSTM.weight_hh_l0)\n",
        "        nn.init.constant(self.aggregation_LSTM.bias_hh_l0, val=0)\n",
        "\n",
        "        nn.init.kaiming_normal(self.aggregation_LSTM.weight_ih_l0_reverse)\n",
        "        nn.init.constant(self.aggregation_LSTM.bias_ih_l0_reverse, val=0)\n",
        "        nn.init.orthogonal(self.aggregation_LSTM.weight_hh_l0_reverse)\n",
        "        nn.init.constant(self.aggregation_LSTM.bias_hh_l0_reverse, val=0)\n",
        "\n",
        "        # ----- Prediction Layer ----\n",
        "        nn.init.uniform(self.pred_fc1.weight, -0.005, 0.005)\n",
        "        nn.init.constant(self.pred_fc1.bias, val=0)\n",
        "\n",
        "        nn.init.uniform(self.pred_fc2.weight, -0.005, 0.005)\n",
        "        nn.init.constant(self.pred_fc2.bias, val=0)\n",
        "\n",
        "    def dropout(self, v):\n",
        "        return F.dropout(v, p=self.args[\"dropout\"], training=self.training)\n",
        "\n",
        "    def forward(self, **kwargs):\n",
        "        # ----- Matching Layer -----\n",
        "        def mp_matching_func(v1, v2, w):\n",
        "            \"\"\"\n",
        "            :param v1: (batch, seq_len, hidden_size)\n",
        "            :param v2: (batch, seq_len, hidden_size) or (batch, hidden_size)\n",
        "            :param w: (l, hidden_size)\n",
        "            :return: (batch, l)\n",
        "            \"\"\"\n",
        "            seq_len = v1.size(1)\n",
        "\n",
        "            # Trick for large memory requirement\n",
        "            \"\"\"\n",
        "            if len(v2.size()) == 2:\n",
        "                v2 = torch.stack([v2] * seq_len, dim=1)\n",
        "\n",
        "            m = []\n",
        "            for i in range(self.l):\n",
        "                # v1: (batch, seq_len, hidden_size)\n",
        "                # v2: (batch, seq_len, hidden_size)\n",
        "                # w: (1, 1, hidden_size)\n",
        "                # -> (batch, seq_len)\n",
        "                m.append(F.cosine_similarity(w[i].view(1, 1, -1) * v1, w[i].view(1, 1, -1) * v2, dim=2))\n",
        "\n",
        "            # list of (batch, seq_len) -> (batch, seq_len, l)\n",
        "            m = torch.stack(m, dim=2)\n",
        "            \"\"\"\n",
        "\n",
        "            # (1, 1, hidden_size, l)\n",
        "            w = w.transpose(1, 0).unsqueeze(0).unsqueeze(0)\n",
        "            # (batch, seq_len, hidden_size, l)\n",
        "            v1 = w * torch.stack([v1] * self.l, dim=3)\n",
        "            if len(v2.size()) == 3:\n",
        "                v2 = w * torch.stack([v2] * self.l, dim=3)\n",
        "            else:\n",
        "                v2 = w * torch.stack([torch.stack([v2] * seq_len, dim=1)] * self.l, dim=3)\n",
        "\n",
        "            m = F.cosine_similarity(v1, v2, dim=2)\n",
        "\n",
        "            return m\n",
        "\n",
        "        def mp_matching_func_pairwise(v1, v2, w):\n",
        "            \"\"\"\n",
        "            :param v1: (batch, seq_len1, hidden_size)\n",
        "            :param v2: (batch, seq_len2, hidden_size)\n",
        "            :param w: (l, hidden_size)\n",
        "            :return: (batch, l, seq_len1, seq_len2)\n",
        "            \"\"\"\n",
        "\n",
        "            # Trick for large memory requirement\n",
        "            \"\"\"\n",
        "            m = []\n",
        "            for i in range(self.l):\n",
        "                # (1, 1, hidden_size)\n",
        "                w_i = w[i].view(1, 1, -1)\n",
        "                # (batch, seq_len1, hidden_size), (batch, seq_len2, hidden_size)\n",
        "                v1, v2 = w_i * v1, w_i * v2\n",
        "                # (batch, seq_len, hidden_size->1)\n",
        "                v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n",
        "                v2_norm = v2.norm(p=2, dim=2, keepdim=True)\n",
        "\n",
        "                # (batch, seq_len1, seq_len2)\n",
        "                n = torch.matmul(v1, v2.permute(0, 2, 1))\n",
        "                d = v1_norm * v2_norm.permute(0, 2, 1)\n",
        "\n",
        "                m.append(div_with_small_value(n, d))\n",
        "\n",
        "            # list of (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, l)\n",
        "            m = torch.stack(m, dim=3)\n",
        "            \"\"\"\n",
        "\n",
        "            # (1, l, 1, hidden_size)\n",
        "            w = w.unsqueeze(0).unsqueeze(2)\n",
        "            # (batch, l, seq_len, hidden_size)\n",
        "            v1, v2 = w * torch.stack([v1] * self.l, dim=1), w * torch.stack([v2] * self.l, dim=1)\n",
        "            # (batch, l, seq_len, hidden_size->1)\n",
        "            v1_norm = v1.norm(p=2, dim=3, keepdim=True)\n",
        "            v2_norm = v2.norm(p=2, dim=3, keepdim=True)\n",
        "\n",
        "            # (batch, l, seq_len1, seq_len2)\n",
        "            n = torch.matmul(v1, v2.transpose(2, 3))\n",
        "            d = v1_norm * v2_norm.transpose(2, 3)\n",
        "\n",
        "            # (batch, seq_len1, seq_len2, l)\n",
        "            m = div_with_small_value(n, d).permute(0, 2, 3, 1)\n",
        "\n",
        "            return m\n",
        "\n",
        "        def attention(v1, v2):\n",
        "            \"\"\"\n",
        "            :param v1: (batch, seq_len1, hidden_size)\n",
        "            :param v2: (batch, seq_len2, hidden_size)\n",
        "            :return: (batch, seq_len1, seq_len2)\n",
        "            \"\"\"\n",
        "\n",
        "            # (batch, seq_len1, 1)\n",
        "            v1_norm = v1.norm(p=2, dim=2, keepdim=True)\n",
        "            # (batch, 1, seq_len2)\n",
        "            v2_norm = v2.norm(p=2, dim=2, keepdim=True).permute(0, 2, 1)\n",
        "\n",
        "            # (batch, seq_len1, seq_len2)\n",
        "            a = torch.bmm(v1, v2.permute(0, 2, 1))\n",
        "            d = v1_norm * v2_norm\n",
        "\n",
        "            return div_with_small_value(a, d)\n",
        "\n",
        "        def div_with_small_value(n, d, eps=1e-8):\n",
        "            # too small values are replaced by 1e-8 to prevent it from exploding.\n",
        "            d = d * (d > eps).float() + eps * (d <= eps).float()\n",
        "            return n / d\n",
        "\n",
        "        # ----- Word Representation Layer -----\n",
        "        # (batch, seq_len) -> (batch, seq_len, word_dim)\n",
        "\n",
        "        p = self.word_emb(kwargs['p'])\n",
        "        h = self.word_emb(kwargs['h'])\n",
        "\n",
        "        if self.args[\"use_char_emb\"]:\n",
        "            # (batch, seq_len, max_word_len) -> (batch * seq_len, max_word_len)\n",
        "            seq_len_p = kwargs['char_p'].size(1)\n",
        "            seq_len_h = kwargs['char_h'].size(1)\n",
        "\n",
        "            char_p = kwargs['char_p'].view(-1, self.args[\"max_word_len\"])\n",
        "            char_h = kwargs['char_h'].view(-1, self.args[\"max_word_len\"])\n",
        "\n",
        "            # (batch * seq_len, max_word_len, char_dim)-> (1, batch * seq_len, char_hidden_size)\n",
        "            _, (char_p, _) = self.char_LSTM(self.char_emb(char_p))\n",
        "            _, (char_h, _) = self.char_LSTM(self.char_emb(char_h))\n",
        "\n",
        "            # (batch, seq_len, char_hidden_size)\n",
        "            char_p = char_p.view(-1, seq_len_p, self.args[\"char_hidden_size\"])\n",
        "            char_h = char_h.view(-1, seq_len_h, self.args[\"char_hidden_size\"])\n",
        "\n",
        "            # (batch, seq_len, word_dim + char_hidden_size)\n",
        "            p = torch.cat([p, char_p], dim=-1)\n",
        "            h = torch.cat([h, char_h], dim=-1)\n",
        "\n",
        "        p = self.dropout(p)\n",
        "        h = self.dropout(h)\n",
        "\n",
        "        # ----- Context Representation Layer -----\n",
        "        # (batch, seq_len, hidden_size * 2)\n",
        "        con_p, _ = self.context_LSTM(p)\n",
        "        con_h, _ = self.context_LSTM(h)\n",
        "\n",
        "        con_p = self.dropout(con_p)\n",
        "        con_h = self.dropout(con_h)\n",
        "\n",
        "        # (batch, seq_len, hidden_size)\n",
        "        con_p_fw, con_p_bw = torch.split(con_p, self.args[\"hidden_size\"], dim=-1)\n",
        "        con_h_fw, con_h_bw = torch.split(con_h, self.args[\"hidden_size\"], dim=-1)\n",
        "\n",
        "        # 1. Full-Matching\n",
        "\n",
        "        # (batch, seq_len, hidden_size), (batch, hidden_size)\n",
        "        # -> (batch, seq_len, l)\n",
        "        mv_p_full_fw = mp_matching_func(con_p_fw, con_h_fw[:, -1, :], self.mp_w1)\n",
        "        mv_p_full_bw = mp_matching_func(con_p_bw, con_h_bw[:, 0, :], self.mp_w2)\n",
        "        mv_h_full_fw = mp_matching_func(con_h_fw, con_p_fw[:, -1, :], self.mp_w1)\n",
        "        mv_h_full_bw = mp_matching_func(con_h_bw, con_p_bw[:, 0, :], self.mp_w2)\n",
        "\n",
        "        # 2. Maxpooling-Matching\n",
        "\n",
        "        # (batch, seq_len1, seq_len2, l)\n",
        "        mv_max_fw = mp_matching_func_pairwise(con_p_fw, con_h_fw, self.mp_w3)\n",
        "        mv_max_bw = mp_matching_func_pairwise(con_p_bw, con_h_bw, self.mp_w4)\n",
        "\n",
        "        # (batch, seq_len, l)\n",
        "        mv_p_max_fw, _ = mv_max_fw.max(dim=2)\n",
        "        mv_p_max_bw, _ = mv_max_bw.max(dim=2)\n",
        "        mv_h_max_fw, _ = mv_max_fw.max(dim=1)\n",
        "        mv_h_max_bw, _ = mv_max_bw.max(dim=1)\n",
        "\n",
        "        # 3. Attentive-Matching\n",
        "\n",
        "        # (batch, seq_len1, seq_len2)\n",
        "        att_fw = attention(con_p_fw, con_h_fw)\n",
        "        att_bw = attention(con_p_bw, con_h_bw)\n",
        "\n",
        "        # (batch, seq_len2, hidden_size) -> (batch, 1, seq_len2, hidden_size)\n",
        "        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n",
        "        # -> (batch, seq_len1, seq_len2, hidden_size)\n",
        "        att_h_fw = con_h_fw.unsqueeze(1) * att_fw.unsqueeze(3)\n",
        "        att_h_bw = con_h_bw.unsqueeze(1) * att_bw.unsqueeze(3)\n",
        "        # (batch, seq_len1, hidden_size) -> (batch, seq_len1, 1, hidden_size)\n",
        "        # (batch, seq_len1, seq_len2) -> (batch, seq_len1, seq_len2, 1)\n",
        "        # -> (batch, seq_len1, seq_len2, hidden_size)\n",
        "        att_p_fw = con_p_fw.unsqueeze(2) * att_fw.unsqueeze(3)\n",
        "        att_p_bw = con_p_bw.unsqueeze(2) * att_bw.unsqueeze(3)\n",
        "\n",
        "        # (batch, seq_len1, hidden_size) / (batch, seq_len1, 1) -> (batch, seq_len1, hidden_size)\n",
        "        att_mean_h_fw = div_with_small_value(att_h_fw.sum(dim=2), att_fw.sum(dim=2, keepdim=True))\n",
        "        att_mean_h_bw = div_with_small_value(att_h_bw.sum(dim=2), att_bw.sum(dim=2, keepdim=True))\n",
        "\n",
        "        # (batch, seq_len2, hidden_size) / (batch, seq_len2, 1) -> (batch, seq_len2, hidden_size)\n",
        "        att_mean_p_fw = div_with_small_value(att_p_fw.sum(dim=1), att_fw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n",
        "        att_mean_p_bw = div_with_small_value(att_p_bw.sum(dim=1), att_bw.sum(dim=1, keepdim=True).permute(0, 2, 1))\n",
        "\n",
        "        # (batch, seq_len, l)\n",
        "        mv_p_att_mean_fw = mp_matching_func(con_p_fw, att_mean_h_fw, self.mp_w5)\n",
        "        mv_p_att_mean_bw = mp_matching_func(con_p_bw, att_mean_h_bw, self.mp_w6)\n",
        "        mv_h_att_mean_fw = mp_matching_func(con_h_fw, att_mean_p_fw, self.mp_w5)\n",
        "        mv_h_att_mean_bw = mp_matching_func(con_h_bw, att_mean_p_bw, self.mp_w6)\n",
        "\n",
        "        # 4. Max-Attentive-Matching\n",
        "\n",
        "        # (batch, seq_len1, hidden_size)\n",
        "        att_max_h_fw, _ = att_h_fw.max(dim=2)\n",
        "        att_max_h_bw, _ = att_h_bw.max(dim=2)\n",
        "        # (batch, seq_len2, hidden_size)\n",
        "        att_max_p_fw, _ = att_p_fw.max(dim=1)\n",
        "        att_max_p_bw, _ = att_p_bw.max(dim=1)\n",
        "\n",
        "        # (batch, seq_len, l)\n",
        "        mv_p_att_max_fw = mp_matching_func(con_p_fw, att_max_h_fw, self.mp_w7)\n",
        "        mv_p_att_max_bw = mp_matching_func(con_p_bw, att_max_h_bw, self.mp_w8)\n",
        "        mv_h_att_max_fw = mp_matching_func(con_h_fw, att_max_p_fw, self.mp_w7)\n",
        "        mv_h_att_max_bw = mp_matching_func(con_h_bw, att_max_p_bw, self.mp_w8)\n",
        "\n",
        "        # (batch, seq_len, l * 8)\n",
        "        mv_p = torch.cat(\n",
        "            [mv_p_full_fw, mv_p_max_fw, mv_p_att_mean_fw, mv_p_att_max_fw,\n",
        "             mv_p_full_bw, mv_p_max_bw, mv_p_att_mean_bw, mv_p_att_max_bw], dim=2)\n",
        "        mv_h = torch.cat(\n",
        "            [mv_h_full_fw, mv_h_max_fw, mv_h_att_mean_fw, mv_h_att_max_fw,\n",
        "             mv_h_full_bw, mv_h_max_bw, mv_h_att_mean_bw, mv_h_att_max_bw], dim=2)\n",
        "\n",
        "        mv_p = self.dropout(mv_p)\n",
        "        mv_h = self.dropout(mv_h)\n",
        "\n",
        "        # ----- Aggregation Layer -----\n",
        "        # (batch, seq_len, l * 8) -> (2, batch, hidden_size)\n",
        "        _, (agg_p_last, _) = self.aggregation_LSTM(mv_p)\n",
        "        _, (agg_h_last, _) = self.aggregation_LSTM(mv_h)\n",
        "\n",
        "        # 2 * (2, batch, hidden_size) -> 2 * (batch, hidden_size * 2) -> (batch, hidden_size * 4)\n",
        "        x = torch.cat(\n",
        "            [agg_p_last.permute(1, 0, 2).contiguous().view(-1, self.args[\"hidden_size\"] * 2),\n",
        "             agg_h_last.permute(1, 0, 2).contiguous().view(-1, self.args[\"hidden_size\"] * 2)], dim=1)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # ----- Prediction Layer -----\n",
        "        x = F.tanh(self.pred_fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.pred_fc2(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_IpdK1mvxUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "from torchtext.vocab import Vectors\n",
        "\n",
        "\n",
        "from nltk import word_tokenize\n",
        "\n",
        "\n",
        "class SNLI():\n",
        "    def __init__(self, args):\n",
        "        self.TEXT = data.Field(batch_first=True, tokenize=word_tokenize, lower=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        self.train, self.dev, self.test = datasets.SNLI.splits(self.TEXT, self.LABEL)\n",
        "        self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=FastText())\n",
        "        self.LABEL.build_vocab(self.train)\n",
        "\n",
        "        self.train_iter, self.dev_iter, self.test_iter = \\\n",
        "            data.BucketIterator.splits((self.train, self.dev, self.test),\n",
        "                                       batch_sizes=[args[\"batch_size\"]] * 3,\n",
        "                                       device=args[\"gpu\"])\n",
        "\n",
        "        self.max_word_len = max([len(w) for w in self.TEXT.vocab.itos])\n",
        "        # for <pad>\n",
        "        self.char_vocab = {'': 0}\n",
        "        # for <unk> and <pad>\n",
        "        self.characterized_words = [[0] * self.max_word_len, [0] * self.max_word_len]\n",
        "\n",
        "        if args[\"use_char_emb\"]:\n",
        "            self.build_char_vocab()\n",
        "\n",
        "    def build_char_vocab(self):\n",
        "        # for normal words\n",
        "        for word in self.TEXT.vocab.itos[2:]:\n",
        "            chars = []\n",
        "            for c in list(word):\n",
        "                if c not in self.char_vocab:\n",
        "                    self.char_vocab[c] = len(self.char_vocab)\n",
        "\n",
        "                chars.append(self.char_vocab[c])\n",
        "\n",
        "            chars.extend([0] * (self.max_word_len - len(word)))\n",
        "            self.characterized_words.append(chars)\n",
        "\n",
        "    def characterize(self, batch):\n",
        "        \"\"\"\n",
        "        :param batch: Pytorch Variable with shape (batch, seq_len)\n",
        "        :return: Pytorch Variable with shape (batch, seq_len, max_word_len)\n",
        "        \"\"\"\n",
        "        batch = batch.data.cpu().numpy().astype(int).tolist()\n",
        "        return [[self.characterized_words[w] for w in words] for words in batch]\n",
        "\n",
        "class Quora():\n",
        "    def __init__(self, args):\n",
        "        self.RAW = data.RawField()\n",
        "        self.TEXT = data.Field(batch_first=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None)\n",
        "\n",
        "        self.train, self.dev, self.test = data.TabularDataset.splits(\n",
        "            path='',\n",
        "            train='tr1.tsv',\n",
        "            validation='dv1.tsv',\n",
        "            test='tt1.tsv',\n",
        "            format='tsv',\n",
        "            fields=[('label', self.LABEL),\n",
        "                    ('q1', self.TEXT),\n",
        "                    ('q2', self.TEXT),\n",
        "                    ('id', self.RAW)])\n",
        "\n",
        "        self.TEXT.build_vocab(self.train, self.dev, self.test, vectors=GloVe(name='840B', dim=300))\n",
        "        self.LABEL.build_vocab(self.train)\n",
        "\n",
        "        self.RAW1 = data.RawField()\n",
        "        self.TEXT1 = data.Field(batch_first=True)\n",
        "        self.LABEL1 = data.Field(sequential=False, unk_token=None)\n",
        "        # vv_vectors = Vectors(name='sense2vec_embedding.vec')\n",
        "        self.train1, self.dev1, self.test1 = data.TabularDataset.splits(\n",
        "            path='',\n",
        "            train='tr.tsv',\n",
        "            validation='dv.tsv',\n",
        "            test='tt.tsv',\n",
        "            format='tsv',\n",
        "            fields=[('label', self.LABEL1),\n",
        "                    ('q1', self.TEXT1),\n",
        "                    ('q2', self.TEXT1),\n",
        "                    ('id', self.RAW1)])\n",
        "        vectors = Vectors(name='sense2vec_embedding.vec', cache='./')\n",
        "        self.TEXT1.build_vocab(self.train1, self.dev1, self.test1, vectors=vectors)\n",
        "        self.LABEL1.build_vocab(self.train)\n",
        "\n",
        "        sort_key = lambda x: data.interleave_keys(len(x.q1), len(x.q2))\n",
        "\n",
        "        self.train_iter, self.dev_iter, self.test_iter = \\\n",
        "            data.BucketIterator.splits((self.train, self.dev, self.test),\n",
        "                                       batch_sizes=[args[\"batch_size\"]] * 3,\n",
        "                                       device=args[\"gpu\"],\n",
        "                                       sort_key=sort_key)\n",
        "\n",
        "        self.max_word_len = max([len(w) for w in self.TEXT.vocab.itos])\n",
        "        # for <pad>\n",
        "        self.char_vocab = {'': 0}\n",
        "        # for <unk> and <pad>\n",
        "        self.characterized_words = [[0] * self.max_word_len, [0] * self.max_word_len]\n",
        "\n",
        "        if args[\"use_char_emb\"]:\n",
        "            self.build_char_vocab()\n",
        "\n",
        "    def build_char_vocab(self):\n",
        "        # for normal words\n",
        "        for word in self.TEXT.vocab.itos[2:]:\n",
        "            chars = []\n",
        "            for c in list(word):\n",
        "                if c not in self.char_vocab:\n",
        "                    self.char_vocab[c] = len(self.char_vocab)\n",
        "\n",
        "                chars.append(self.char_vocab[c])\n",
        "\n",
        "            chars.extend([0] * (self.max_word_len - len(word)))\n",
        "            self.characterized_words.append(chars)\n",
        "\n",
        "    def characterize(self, batch):\n",
        "        \"\"\"\n",
        "        :param batch: Pytorch Variable with shape (batch, seq_len)\n",
        "        :return: Pytorch Variable with shape (batch, seq_len, max_word_len)\n",
        "        \"\"\"\n",
        "        batch = batch.data.cpu().numpy().astype(int).tolist()\n",
        "        return [[self.characterized_words[w] for w in words] for words in batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOPXpfX1xoIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import argparse\n",
        "import copy\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.autograd import Variable\n",
        "from tensorboardX import SummaryWriter\n",
        "from time import gmtime, strftime\n",
        "\n",
        "\n",
        "def train(args, data):\n",
        "    model = BIMPM(args, data)\n",
        "    if args[\"gpu\"] > -1:\n",
        "        model.cuda(args[\"gpu\"])\n",
        "\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adam(parameters, lr=args[\"learning_rate\"])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    writer = SummaryWriter(log_dir='runs/' + args[\"model_time\"])\n",
        "\n",
        "    model.train()\n",
        "    loss, last_epoch = 0, -1\n",
        "    max_dev_acc, max_test_acc = 0, 0\n",
        "\n",
        "    iterator = data.train_iter\n",
        "    for i, batch in enumerate(iterator):\n",
        "        present_epoch = int(iterator.epoch)\n",
        "        if present_epoch == args[\"epoch\"]:\n",
        "            break\n",
        "        if present_epoch > last_epoch:\n",
        "            print('epoch:', present_epoch + 1)\n",
        "        last_epoch = present_epoch\n",
        "\n",
        "        if args[\"data_type\"] == 'SNLI':\n",
        "            s1, s2 = 'premise', 'hypothesis'\n",
        "        else:\n",
        "            s1, s2 = 'q1', 'q2'\n",
        "\n",
        "        s1, s2 = getattr(batch, s1), getattr(batch, s2)\n",
        "\n",
        "        # limit the lengths of input sentences up to max_sent_len\n",
        "        if args[\"max_sent_len\"] >= 0:\n",
        "            if s1.size()[1] > args[\"max_sent_len\"]:\n",
        "                s1 = s1[:, :args[\"max_sent_len\"]]\n",
        "            if s2.size()[1] > args[\"max_sent_len\"]:\n",
        "                s2 = s2[:, :args[\"max_sent_len\"]]\n",
        "\n",
        "        kwargs = {'p': s1, 'h': s2}\n",
        "\n",
        "        if args[\"use_char_emb\"]:\n",
        "            char_p = Variable(torch.LongTensor(data.characterize(s1)))\n",
        "            char_h = Variable(torch.LongTensor(data.characterize(s2)))\n",
        "\n",
        "            if args[\"gpu\"] > -1:\n",
        "                char_p = char_p.cuda(args[\"gpu\"])\n",
        "                char_h = char_h.cuda(args[\"gpu\"])\n",
        "\n",
        "            kwargs['char_p'] = char_p\n",
        "            kwargs['char_h'] = char_h\n",
        "\n",
        "        pred = model(**kwargs)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = criterion(pred, batch.label)\n",
        "        loss += batch_loss.data[0]\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i + 1) % args[\"print_freq\"] == 0:\n",
        "            dev_loss, dev_acc = test(model, args, data, mode='dev')\n",
        "            test_loss, test_acc = test(model, args, data)\n",
        "            c = (i + 1) // args[\"print_freq\"]\n",
        "\n",
        "            writer.add_scalar('loss/train', loss, c)\n",
        "            writer.add_scalar('loss/dev', dev_loss, c)\n",
        "            writer.add_scalar('acc/dev', dev_acc, c)\n",
        "            writer.add_scalar('loss/test', test_loss, c)\n",
        "            writer.add_scalar('acc/test', test_acc, c)\n",
        "\n",
        "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f} / test loss: {test_loss:.3f}'\n",
        "                  f' / dev acc: {dev_acc:.3f} / test acc: {test_acc:.3f}')\n",
        "\n",
        "            if dev_acc > max_dev_acc:\n",
        "                max_dev_acc = dev_acc\n",
        "                max_test_acc = test_acc\n",
        "                best_model = copy.deepcopy(model)\n",
        "\n",
        "            loss = 0\n",
        "            model.train()\n",
        "\n",
        "    writer.close()\n",
        "    print(f'max dev acc: {max_dev_acc:.3f} / max test acc: {max_test_acc:.3f}')\n",
        "\n",
        "    return best_model\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = {}\n",
        "    args['batch_size'] = 16\n",
        "    args['char_dim'] =  20\n",
        "    args['char_hidden_size']=50\n",
        "    args['data_type']='Quora'\n",
        "    args['dropout'] = 0.1\n",
        "    args['epoch'] = 10\n",
        "    args['gpu'] =0\n",
        "    args['hidden_size'] =100\n",
        "    args['learning_rate'] =0.001\n",
        "    args['max_sent_len'] =-1\n",
        "    args['num_perspective'] =20\n",
        "    args['print_freq'] =500\n",
        "    args['use_char_emb'] =True\n",
        "    args['word_dim'] = 300\n",
        "\n",
        "    if args[\"data_type\"] == 'SNLI':\n",
        "        print('loading SNLI data...')\n",
        "        data = SNLI(args)\n",
        "    elif args[\"data_type\"] == 'Quora':\n",
        "        print('loading Quora data...')\n",
        "        data = Quora(args)\n",
        "    else:\n",
        "        raise NotImplementedError('only SNLI or Quora data is possible')\n",
        "\n",
        "    args['char_vocab_size']= len(data.char_vocab)\n",
        "    args['word_vocab_size']= len(data.TEXT1.vocab)\n",
        "    args['class_size'] = len(data.LABEL.vocab)\n",
        "    args['max_word_len'] = data.max_word_len\n",
        "    args['model_time'] = strftime('%H:%M:%S', gmtime())\n",
        "\n",
        "    print('training start!')\n",
        "    best_model = train(args, data)\n",
        "\n",
        "    if not os.path.exists('saved_models'):\n",
        "        os.makedirs('saved_models')\n",
        "    torch.save(best_model.state_dict(), f'saved_models/BIBPM_{args[\"data_type\"]}_{args[\"model_time\"]}.pt')\n",
        "\n",
        "    print('training finished!')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epiz3-FzQHBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhNJR0a1xz96",
        "colab_type": "code",
        "outputId": "846460a1-c135-42ef-a482-4f022672a47b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-21556c55-fc33-4a68-8ee3-e7e75992b0a6\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-21556c55-fc33-4a68-8ee3-e7e75992b0a6\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving sense2vec_embedding.vec to sense2vec_embedding.vec\n",
            "Saving tt.tsv to tt.tsv\n",
            "Saving dv.tsv to dv.tsv\n",
            "Saving tr.tsv to tr.tsv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S39aSCpByc7X",
        "colab_type": "code",
        "outputId": "244ddb32-0b26-4f2d-c48e-f90a3cc4ce3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading Quora data...\n",
            "training start!\n",
            "epoch: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: generator 'Iterator.__iter__' raised StopIteration\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "train loss: 355.280 / dev loss: 129.720 / test loss: 129.424 / dev acc: 0.550 / test acc: 0.550\n",
            "epoch: 2\n",
            "train loss: 337.134 / dev loss: 125.035 / test loss: 124.489 / dev acc: 0.595 / test acc: 0.606\n",
            "train loss: 324.886 / dev loss: 123.676 / test loss: 123.299 / dev acc: 0.612 / test acc: 0.624\n",
            "epoch: 3\n",
            "train loss: 317.708 / dev loss: 122.717 / test loss: 122.399 / dev acc: 0.616 / test acc: 0.621\n",
            "train loss: 308.510 / dev loss: 121.926 / test loss: 120.436 / dev acc: 0.621 / test acc: 0.631\n",
            "epoch: 4\n",
            "train loss: 300.867 / dev loss: 124.803 / test loss: 123.493 / dev acc: 0.634 / test acc: 0.635\n",
            "train loss: 295.813 / dev loss: 116.029 / test loss: 115.461 / dev acc: 0.658 / test acc: 0.656\n",
            "epoch: 5\n",
            "train loss: 296.935 / dev loss: 117.480 / test loss: 116.328 / dev acc: 0.648 / test acc: 0.656\n",
            "train loss: 287.815 / dev loss: 117.322 / test loss: 116.532 / dev acc: 0.654 / test acc: 0.666\n",
            "epoch: 6\n",
            "train loss: 283.900 / dev loss: 118.375 / test loss: 117.535 / dev acc: 0.651 / test acc: 0.655\n",
            "train loss: 280.194 / dev loss: 118.218 / test loss: 116.735 / dev acc: 0.654 / test acc: 0.661\n",
            "epoch: 7\n",
            "train loss: 276.977 / dev loss: 121.460 / test loss: 121.139 / dev acc: 0.664 / test acc: 0.652\n",
            "train loss: 271.400 / dev loss: 120.525 / test loss: 120.019 / dev acc: 0.658 / test acc: 0.656\n",
            "epoch: 8\n",
            "train loss: 262.308 / dev loss: 120.805 / test loss: 119.117 / dev acc: 0.660 / test acc: 0.656\n",
            "train loss: 268.353 / dev loss: 119.797 / test loss: 117.610 / dev acc: 0.662 / test acc: 0.666\n",
            "epoch: 9\n",
            "train loss: 253.557 / dev loss: 124.038 / test loss: 123.310 / dev acc: 0.647 / test acc: 0.646\n",
            "epoch: 10\n",
            "train loss: 256.601 / dev loss: 127.537 / test loss: 125.346 / dev acc: 0.653 / test acc: 0.657\n",
            "train loss: 250.122 / dev loss: 121.041 / test loss: 119.413 / dev acc: 0.655 / test acc: 0.652\n",
            "epoch: 11\n",
            "train loss: 248.970 / dev loss: 127.725 / test loss: 124.438 / dev acc: 0.664 / test acc: 0.653\n",
            "train loss: 244.156 / dev loss: 131.579 / test loss: 129.179 / dev acc: 0.651 / test acc: 0.656\n",
            "epoch: 12\n",
            "train loss: 237.379 / dev loss: 131.330 / test loss: 129.562 / dev acc: 0.663 / test acc: 0.661\n",
            "train loss: 237.225 / dev loss: 130.198 / test loss: 128.388 / dev acc: 0.665 / test acc: 0.665\n",
            "epoch: 13\n",
            "train loss: 233.516 / dev loss: 133.408 / test loss: 130.880 / dev acc: 0.652 / test acc: 0.656\n",
            "train loss: 227.788 / dev loss: 129.389 / test loss: 127.605 / dev acc: 0.657 / test acc: 0.653\n",
            "epoch: 14\n",
            "train loss: 223.675 / dev loss: 134.097 / test loss: 130.425 / dev acc: 0.662 / test acc: 0.667\n",
            "train loss: 226.974 / dev loss: 128.594 / test loss: 127.122 / dev acc: 0.657 / test acc: 0.660\n",
            "epoch: 15\n",
            "train loss: 219.222 / dev loss: 137.347 / test loss: 138.359 / dev acc: 0.635 / test acc: 0.646\n",
            "train loss: 222.677 / dev loss: 130.308 / test loss: 128.901 / dev acc: 0.659 / test acc: 0.662\n",
            "epoch: 16\n",
            "train loss: 210.741 / dev loss: 134.224 / test loss: 131.328 / dev acc: 0.654 / test acc: 0.662\n",
            "train loss: 212.824 / dev loss: 140.989 / test loss: 139.777 / dev acc: 0.664 / test acc: 0.652\n",
            "epoch: 17\n",
            "train loss: 200.901 / dev loss: 134.812 / test loss: 132.825 / dev acc: 0.665 / test acc: 0.661\n",
            "epoch: 18\n",
            "train loss: 204.851 / dev loss: 157.041 / test loss: 151.829 / dev acc: 0.651 / test acc: 0.658\n",
            "train loss: 200.443 / dev loss: 145.087 / test loss: 138.375 / dev acc: 0.643 / test acc: 0.663\n",
            "epoch: 19\n",
            "train loss: 201.360 / dev loss: 144.854 / test loss: 141.333 / dev acc: 0.655 / test acc: 0.661\n",
            "train loss: 196.639 / dev loss: 144.797 / test loss: 143.725 / dev acc: 0.648 / test acc: 0.652\n",
            "epoch: 20\n",
            "train loss: 195.697 / dev loss: 152.884 / test loss: 145.403 / dev acc: 0.644 / test acc: 0.655\n",
            "train loss: 187.916 / dev loss: 149.000 / test loss: 143.364 / dev acc: 0.660 / test acc: 0.663\n",
            "epoch: 21\n",
            "train loss: 187.223 / dev loss: 148.931 / test loss: 143.284 / dev acc: 0.655 / test acc: 0.661\n",
            "train loss: 195.091 / dev loss: 148.446 / test loss: 142.258 / dev acc: 0.655 / test acc: 0.652\n",
            "epoch: 22\n",
            "train loss: 181.953 / dev loss: 156.354 / test loss: 152.246 / dev acc: 0.653 / test acc: 0.642\n",
            "train loss: 181.658 / dev loss: 142.824 / test loss: 139.739 / dev acc: 0.662 / test acc: 0.664\n",
            "epoch: 23\n",
            "train loss: 180.784 / dev loss: 163.554 / test loss: 159.336 / dev acc: 0.650 / test acc: 0.653\n",
            "train loss: 182.130 / dev loss: 171.359 / test loss: 166.100 / dev acc: 0.658 / test acc: 0.658\n",
            "epoch: 24\n",
            "train loss: 178.995 / dev loss: 169.818 / test loss: 166.664 / dev acc: 0.658 / test acc: 0.650\n",
            "train loss: 178.804 / dev loss: 160.893 / test loss: 157.303 / dev acc: 0.655 / test acc: 0.642\n",
            "epoch: 25\n",
            "train loss: 166.554 / dev loss: 153.437 / test loss: 146.985 / dev acc: 0.653 / test acc: 0.665\n",
            "epoch: 26\n",
            "train loss: 179.249 / dev loss: 173.241 / test loss: 168.303 / dev acc: 0.657 / test acc: 0.654\n",
            "train loss: 167.998 / dev loss: 165.600 / test loss: 160.890 / dev acc: 0.659 / test acc: 0.669\n",
            "epoch: 27\n",
            "train loss: 167.585 / dev loss: 166.658 / test loss: 167.036 / dev acc: 0.668 / test acc: 0.659\n",
            "train loss: 168.781 / dev loss: 157.918 / test loss: 155.363 / dev acc: 0.653 / test acc: 0.663\n",
            "epoch: 28\n",
            "train loss: 168.473 / dev loss: 171.448 / test loss: 165.892 / dev acc: 0.655 / test acc: 0.661\n",
            "train loss: 160.532 / dev loss: 174.823 / test loss: 168.043 / dev acc: 0.659 / test acc: 0.662\n",
            "epoch: 29\n",
            "train loss: 160.504 / dev loss: 171.465 / test loss: 167.057 / dev acc: 0.654 / test acc: 0.662\n",
            "train loss: 161.225 / dev loss: 168.926 / test loss: 161.490 / dev acc: 0.651 / test acc: 0.654\n",
            "epoch: 30\n",
            "train loss: 160.476 / dev loss: 186.754 / test loss: 175.162 / dev acc: 0.647 / test acc: 0.654\n",
            "train loss: 159.458 / dev loss: 171.239 / test loss: 161.729 / dev acc: 0.645 / test acc: 0.662\n",
            "epoch: 31\n",
            "train loss: 153.739 / dev loss: 177.900 / test loss: 171.691 / dev acc: 0.656 / test acc: 0.663\n",
            "train loss: 160.074 / dev loss: 168.933 / test loss: 165.412 / dev acc: 0.662 / test acc: 0.664\n",
            "epoch: 32\n",
            "train loss: 153.453 / dev loss: 171.804 / test loss: 165.701 / dev acc: 0.656 / test acc: 0.652\n",
            "train loss: 155.968 / dev loss: 166.928 / test loss: 161.365 / dev acc: 0.645 / test acc: 0.666\n",
            "epoch: 33\n",
            "train loss: 149.163 / dev loss: 172.849 / test loss: 167.983 / dev acc: 0.655 / test acc: 0.656\n",
            "epoch: 34\n",
            "train loss: 157.513 / dev loss: 185.247 / test loss: 180.063 / dev acc: 0.661 / test acc: 0.661\n",
            "train loss: 146.864 / dev loss: 177.145 / test loss: 174.224 / dev acc: 0.671 / test acc: 0.665\n",
            "epoch: 35\n",
            "train loss: 153.895 / dev loss: 191.366 / test loss: 189.554 / dev acc: 0.661 / test acc: 0.656\n",
            "train loss: 146.222 / dev loss: 185.020 / test loss: 178.240 / dev acc: 0.665 / test acc: 0.665\n",
            "epoch: 36\n",
            "train loss: 148.010 / dev loss: 167.125 / test loss: 163.255 / dev acc: 0.656 / test acc: 0.664\n",
            "train loss: 144.785 / dev loss: 173.235 / test loss: 169.365 / dev acc: 0.660 / test acc: 0.660\n",
            "epoch: 37\n",
            "train loss: 143.491 / dev loss: 198.726 / test loss: 192.405 / dev acc: 0.654 / test acc: 0.653\n",
            "train loss: 148.881 / dev loss: 177.117 / test loss: 173.959 / dev acc: 0.652 / test acc: 0.653\n",
            "epoch: 38\n",
            "train loss: 142.745 / dev loss: 203.124 / test loss: 193.656 / dev acc: 0.659 / test acc: 0.665\n",
            "train loss: 142.448 / dev loss: 196.934 / test loss: 192.758 / dev acc: 0.663 / test acc: 0.650\n",
            "epoch: 39\n",
            "train loss: 140.085 / dev loss: 187.678 / test loss: 186.649 / dev acc: 0.645 / test acc: 0.648\n",
            "train loss: 141.672 / dev loss: 182.089 / test loss: 181.146 / dev acc: 0.651 / test acc: 0.644\n",
            "epoch: 40\n",
            "train loss: 136.669 / dev loss: 212.554 / test loss: 211.158 / dev acc: 0.660 / test acc: 0.653\n",
            "train loss: 142.939 / dev loss: 181.555 / test loss: 172.894 / dev acc: 0.651 / test acc: 0.671\n",
            "max dev acc: 0.671 / max test acc: 0.665\n",
            "training finished!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwzrNmZ7c9bK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}